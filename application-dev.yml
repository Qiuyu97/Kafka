# ===================================================================
# Spring Boot configuration for the "dev" profile.
#
# This configuration overrides the application.yml file.
#
# More information on profiles: https://www.jhipster.tech/profiles/
# More information on configuration properties: https://www.jhipster.tech/common-application-properties/
# ===================================================================

# ===================================================================
# Standard Spring Boot properties.
# Full reference is available at:
# http://docs.spring.io/spring-b’；oot/docs/current/reference/html/common-application-properties.html
# ===================================================================

##   参考Kafka配置文件 

spring:
    # 2020-11-23 添加集成kafka S
    kafka:
        #bootstrap-servers: 10.164.27.72:9092,10.164.27.73:9093,10.164.27.74:9094
        bootstrap-servers: 192.168.0.100:9092
        producer:
            # 发生错误后，消息重发的次数。
            retries: 0
            #当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。
            batch-size: 16384
            # 设置生产者内存缓冲区的大小。
            buffer-memory: 33554432
            # 键的序列化方式
            key-serializer: org.apache.kafka.common.serialization.StringSerializer
            # 值的序列化方式
            value-serializer: org.apache.kafka.common.serialization.StringSerializer
            # acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。
            # acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。
            # acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。
            acks: 1
        consumer:
            # 自动提交的时间间隔 在spring boot 2.X 版本中这里采用的是值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5D
            auto-commit-interval: 1S
            # 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：
            # latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）
            # earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录
            auto-offset-reset: earliest
            # 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量
            enable-auto-commit: false
            # 键的反序列化方式
            key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
            # 值的反序列化方式
            value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
            #Consumer每次调用poll()时取到的records的最大数。
            max-poll-records: 20000
            #(默认 1MB) – 消费者能读取的最大消息。这个值应该大于或等于kafka服务的最大接收字节数message.max.bytes
            max.partition.fetch.bytes: 1048576
            #Consumer进程的标识。如果设置一个人为可读的值，跟踪问题会比较方便。
            client-id: ncomp-client
        listener:
            # 在侦听器容器中运行的线程数。
            #每个consumer起的处理消息的核心线程数，跟consumer节点多少关系不大
            concurrency: 3
            #listner负责ack，每调用一次，就立即commit
            ack-mode: manual_immediate
            missing-topics-fatal: false
    # 2020-11-23 添加集成kafka E

 